\chapter{Introduction}
\label{chap:introduction}

Resource requirements of typical workloads in modern applications well surpass the
capacity of a single machine. System designers distribute applications among clusters of
machines to
meet the storage and computation requirements of these workloads. Therefore
designing systems that can scale across many machines becomes important as
resource requirements increase over time.

In a distributed application, data access locality is a performance goal
and load balancing is a closely related condition.
As the workloads of the distributed applications grow, the application
requests might need to be processed on multiple machines based on how the 
required data is distributed throughout the cluster. The application instances
can communicate using well known techniques like message passing, however
message passing or
distributed shared memory are too general for some applications. For example
MapReduce \cite{dean2008mapreduce} can rarely benefit from the privilege of
access to a global
shared memory in a synchronized manner. These are the applications with
data that is partitionable such that the data accesses from one operation
fall in that partition only. This is desirable because as long as we
satisfy the load balancing constraint, we can schedule the computations
such that they will have local access to their data.

In each of the following examples a form of partitioning is used to limit
cross-partition accesses, while allowing those partitions to be
distributed evenly on multiple machines.

        \subparagraph{Randomly partitioned in-memory distributed hash tables:} One partition or shard
    does not require any information from other partitions to be able to
    operate. We need to route a request to the machine that contains the
    corresponding shard,
    and pass the request (e.g. get or set) to it. The
    request will be carried out internally by the shard on the data that
    is owned by the shard and is local to the machine.

    \subparagraph{Partitioning shipping orders by warehouse IDs:} during the
    processing of an order, there might be multiple sub-systems involved,
    but the inventory and stock of each warehouse, are internal to
    the warehouse partitions. Those parts of the requests that concern the
    stock and inventory can be processed internally by the shard without
    communication with other partitions.

    \subparagraph{Web application user behavior statistics:} We can partition
    the data and the supported operations (append user action,
    query user actions, etc.) by user IDs.
    For most of
    the user actions, the operations can be done by local access to the
    data structures in the current user partition. To prevent hotspots when
    storing historical data, we can aim for finer grain partitioning using
    multiple keys. In this case we can
    partition on user IDs and the date of the action, however operations like
    querying the behavior of the user over the past
    week will end up with more cross-partition accesses than the previous
    sharding strategy well all of the data of a user lies inside one shard.

\paragraph{}
Most of these applications have specific data access or data sharing patterns.
These patterns are results of designing specialized systems for specific
use cases like key value stores or per user statistics.
As a result, systems designers come up with specialized data sharing methods
which take advantage of these conditions and by designing the algorithms
in the system based on these methods they achieve better performance compared
to designing the algorithms without taking into account useful properties of
how application data is managed.

This process accomplishes the goal of performance improvement, but propagates
the complexity of programming for limited data access and sharing patterns
down to the level of the core algorithms in the system. As an example, suppose
one machine in the above distributed hash table cluster becomes a hotspot,
as a result of multiple hot partitions residing on it. One way of
remedying this situation would be for the system to somehow stop the incoming
flow of requests to one of the hot shards, serialize that shard, send it to
a machine with less load, and re-route all the requests to that node. There
are multiple issues with approaches similar to this. First, the shard will be
inaccessible for a long period while it is being transferred. In addition
to that,
the serialization incurs a considerable performance penalty. This increases
code complexity too as the shard entity must have migration built into it,
a requirement which can interfere with the flexibility of the internal
algorithms of the hash table.

Design decisions like the above make applications harder to maintain. Their
core algorithms will become dependent on low level system properties.
Replacing the
algorithm implementation with more efficient versions developed by others will
be difficult. In such an application
the algorithms and data structures which the system uses are no longer well
defined black boxes. As a result, the overall design of the system will be
closed to additions, and we will be stuck with very restricted APIs and specific
guarantees on how they work, while trying not to voilate our own data management
conditions.

\ANGELA{Good problem statement. Not clear however what it means for data
partitioning to be well-defined.}
\CHECK{Rewrote introduction to make data partitioning understandable and make
the problem more clear.}
In general terms, the above situation arises when system designers design
core algorithms with data partitioning in mind, providing solutions for two
problems at the same time. The issue is that in doing so, they typically have to
trade-off programmability and maintainability, while making their overall design
more complex.

We can therefore ask if it is possible to reach a balance in the above equation.
Is it possible to select a subset of partitionable application types
and provide well defined machinery for their data sharing? If this data sharing
framework supports moving the partitions between the machines with minimal
interference with the algorithms' execution inside the partition, without
sacrificing performance (e.g. with serialization and deserialization), the
application designers can comfortably design \emph{single partitions}, instead
of \emph{whole distributed systems}. The platform will then ensure that each
partition can run correctly, and that the resources on no machine are
over-subscribed. Not only the application will achieve load balancing without
much effort,
but also it will end up with a simpler design, with all
of the logic for a distributed environment being managed by the framework,
out of the core application code.

% \TODO{How do I write something general about data locality and load imbalance?}
% The use of in-memory data processing systems has recently grown as a result of
% different advances in computer hardware. Lower operating cost of DRAM means we
% are incentivized to design systems that sacrifice memory usage for performance.
% Faster network interconnects reduce the latency of communication between
% machines, minimizing the performance cost of scaling out server software.
% Analogically similar to the above, persistent memory tightens the performance
% gap between in-memory data processing and file system persistence layer,
% allowing applications to persist data structures as fast as they can use them
% in main memory, as they are kept consistent across operations.
%
% \TODO{citations?}
% these improvements have collectively and independently cast doubt on the precondition of
% ``slow devices, fast CPU'' by gradually shifting both latency and throughput
% bottlenecks towards the classical position of the processor and the kernel.
% This had encouraged research towards building systems which manage their
% network communication and storage more closely. User space networking \TODO{ cite
% DPDK and/or the likes and things that use them? what else?} and managing high
% traffic data structures like hash-tables and balanced binary search trees
% \TODO{mention masstree or similar, and some usages?} in the application are
% both research trends in developing what we call \emph{data structure centric}
% applications \TODO{citations/examples?}. Indeed the operating system kernel provides networking and high
% level interface to useful data structures (e.g. through the interface of file
% systems) but for high performance applications, the performance gain from using
% a specialized way of managing data is worth the complexity.
% Some of these systems target low latency, but all of them have to provide
% a solution for higher throughput workloads. As we exhaust the resource limits
% of a single machine, this becomes synonymous to how the system scales out to
% multiple machines.\TODO{suitable point to break the paragraph?}
% 
% In multi-machine environments, any non-trivial (read-only in essence) system
% faces problems that also become apparent when we cross the single-threaded
% to multi-threaded boundary: how to manage accesses to shared data and where to
% schedule computations. These in part translate to two separate objectives in
% the design of data structure centric distributed systems. \TODO{should I use a
% hyphen in there?} \TODO{is centric a word?}
% \begin{itemize}
%     \item How to keep computation close to the data that it uses (Locality)
%     \item Where to process each request to avoid overloading any of the machines (Load balancing)
% \end{itemize}
% 
% 
% 
% 
% 
% \section{Locality and load balancing}
% System designers have came up with various solutions to to keep their
% computations as close as possible to their data.
% 
% Some applications can tolerate relaxed guarantees for how reads and writes are
% executed. \TODO{mention a few examples and their requirements?}
% Typically replicas are or can be used in these applications as they can be
% used to process certain (particularly read-only) requests based on the
% consistency requirements of the system.
% 
% Not paying attention to the locality constraints and remote access penalties
% and simply contending for resources results (e.g. optimistic concurrency
% control) will result in poor performance in workloads with highly contended
% accesses.
% 
% \begin{figure}[hbt!]
% 
% 
% \begin{tikzpicture}[baseline]
% \begin{axis}[
% width=7cm,
% xmin=1,xmax=28,
% ymin= 0,ymax=1400,
% xlabel=Number of writer threads,ylabel=Average write latency(cycles),
% xtick={1,4,8,...,28},
% ytick={1,200,400,...,1400},
% ]
% \addplot coordinates{
%   (1,9) (2,65) (3,91) (4,139) (5,194) (6,278) (7,299) (8,394) (9,456) (10,425)
%   (11,464) (12,505) (13,546) (14,592) (15,699) (16,688) (17,732) (18,849)
%   (19,820) (20,866) (21,1058) (22,990) (23,1046) (24,1090) (25,1225) (26,1251)
%   (27,1308) (28,1318)
% };
% \end{axis}
% \end{tikzpicture}%
% ~%
% %
% \begin{tikzpicture}[baseline]
% \begin{axis}[
% width=7cm,
% % xmin=0,xmax=32,
% % ymin= 0,ymax=0.10,
% xlabel=Number of worker coroutines per thread,ylabel=Successful transactions,
% xtick={1,5,10,20,30},
% ytick={200000,400000,600000,800000,1000000},
% ]
% \addplot coordinates{
%     (1,730000) (5,870000) (10,600000) (20,200000) (30,50000)
% };
% \end{axis}
% \end{tikzpicture}%
% 
% \caption{
%     The left figure, similar to one from \cite{boyd2014oplog}, shows how
%     performance degrades as different cores contend on a lock for synchronized
%     write access to a shared byte. To remedy this, they propose a batching
%     technique. The right figure shows how increasing
%     the number of coroutines in a distributed transaction processing system
%     \cite{kalia2016fasst} impacts performance on a contended workload.
%       }
% \label{fig:contention_penalty}
% 
% \end{figure}
% 
% Sharding techniques are also used extensively, allowing system designers to
% choose almost any point in the spectrum of design space between high-locality;
% coarser load balancing (e.g. partitioning user data by user id) and
% low-locality; better load balancing (e.g. random sharding).
% 
% An important property of some of these designs is modularity, as
% a result of the problem space being composable, with similarities to how the
% memory
% hierarchy is laid out and how similar problems arise at every level. For
% example all of the above remain
% relevant even after one ``layer'' of randomly sharding the data or the requests,
% as the designer of the system is yet to decide what is the consistency
% requirements of the system within and in between the shards, as well as
% enjoying the freedom of applying other techniques in the higher layers.
% This suggests that
% it would make sense for the solutions targeting locality and
% load balancing to attend to a particular horizontal slice of the problem,
% limiting the conditions and requirements. For example a high performance
% locking scheme can choose to provide the flexibility to be incorporated in a
% sharding scheme to manage cross-partition accesses, as opposed to being bound
% be used in a specific partitioning scenario.
% Not only this guideline
% will make the design of systems simpler and more effective, it allows us to
% fuse independent solutions for different locality and load balancing
% sub-problems when designing a system.
% 
% With that in mind, we have limited our target domain to the ``layer'' where each
% subset of read and write requests can be encapsulated with the data structures
% that they need to access, creating a scheduling unit which along with its
% respective computation, will be placed on a single machine. The way the
% application organizes its workload into these units (upper design layers)
% and what the system decides to do inside each one of these units (lower design
% layers), as well as how these units communicate, will be left to the
% application, which will be responsible for all the plumbing, with the gained
% benefit of composability of different solutions.
% 
% 
% \TODO{Is this section full of useless jargon? Are there too quick shifts from general hand waving
% to specific details? Does it get a message through? Do we need to change the
% section title? Would a figure be useful?}

\section{Contributions}
To provide an answer to the above, we extend the notion of migratability that is
introduced in RAMP \cite{memon2018ramp} to data structures and in general,
self-containing units of work in distributed applications.
We introduce
the properties of migration-friendly applications and discuss
how they would interface with an object-based migration platform.
We then present the design of Slope, an object-based migration framework
and its implementation in C++ using RDMA networking and propose metrics
to measure the performance of migration frameworks based on how they affect
the applications.
To conclude, we provide an overview of the shortcomings of Slope and
discuss possible future directions to improve the migratable programming model.

