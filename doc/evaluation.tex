\chapter{Evaluation}
\label{chap:evaluation}

We go over a few use cases of Slope in real-world systems and present both
micro-benchmarks and full benchmarks for select applications. We also discuss
the metrics that the applications using Slope can measure to get a sense of
how much Slope is impacting their performance.

To measure times and calculate performance metrics globally in the cluster,
we synchronize the start time from one machine to all other machines in the
cluster, by estimating the round trip time between them, which we do by
calculating the median among multiple round trip time calculations. Round
trip times are approximately $5 {\mu}{s}$, a pessimistic upper bound for the
error in time synchronization.

To make sure our graphs are accurate, we run each configuration at least 5 times
and average out the results, and in some cases, drop the min and max values to
eliminate the outlying points. That means each point in each of our graphs is
the resulting value from the above calculation on multiple runs with the same
configuration. Even without eliminating the outlying points, the standard errors
of our measurements are negligible (i.e. multiple orders of magnitude smaller)
compared to the reported values, unless otherwise stated.

Looking at each sub-system in Slope, one can define certain metrics that
reflect if that sub-system is working efficiently. For example we measure
the elapsed time until the prefill operation completes. We also measure
other metrics which more directly impact application performance, such as
end to end migration delay or time during which the object is unusable at
either end.

\subsection{Migration friendliness of data structures}
Based on how the migration process and specifically how the prefill operation
works, objects which use their internal memory in a ``fixed'' manner, that is
without doing much memory allocation/deallocation, are very good candidates
for migration, since they can function seamlessly throughout the prefill phase,
by only inducing dirty page overhead.

Examples of these objects include bloom filters, where we have a fixed
array of bits the size of which always stays the same.
Similarly hash tables which use open addressing techniques such as cuckoo
hashing scheme for their collision resolution are also good candidates for the
same reason.
Apart from these objects which make the best-case scenario for Slope, we also
discuss more generic objects whose allocation/deallocation patterns are not
ideal.

\section{Case study: core metrics and STL objects}
\subsection{Migrating a static vector}

\begin{figure}[tp]
    \begin{center}
        \input{bench-readonly.pgf}
    \end{center}
    \caption{Read-only object migration statistics}
    \label{fig:vectorreadonly}
\end{figure}

In our simplest example, we create a \texttt{vector}, initialize it with the
pre-specified size and migrate it to the destination. Approximately $8$ lines
of code are required on each of the source and destination sides to reproduce
this operation, excluding lines that serve the purpose of gathering statistics.
\autoref{fig:vectorreadonly} depicts the results.

Naturally, the prefill phase takes up most of the transfer time, which grows
linearly by increasing the size of the object. The time it takes to turn over
the ownership, during which the object is unusable on either side is multiple
orders of magnitude smaller than the total time migration time.
It grows faster than linear because we call into the memory allocator
for each 4KB page. The memory allocator uses a red-black tree internally which
increases the running time by a factor of $log n$. We also call into the
operating system through \texttt{mprotect} for each page which contributes
to the running time.

As discussed previously, these can be optimized out by
merging adjacent ranges to each other, yielding close to constant running time
in this case, however we removed this optimization to allow for a better
comparison to be made with the more complicated (e.g. read-write) cases or
with read-only scenarios,
with more realistic allocation/deallocation behaviors, causing fragmentation
of the object memory.

\subsection{Migrating a write-heavy object}
while all of its pages are being written to

plot elapsed times w.r.t. number of pages the object spans

\subsection{Migrating an object based on the writes it receives}
Object size from 10 to ... each of them are rows in the same plot

x axis shows the percentage of the pages that are constantly being dirtied

Each point shows the elapsed time of that particular setting


\section{Case study: migration friendly objects}

bloom filter

cucko hashing

double hashing

\section{Case study: generic objects}

c++ map



% \TODO{this model encourages fixed objects, where you don't need to allocate repeatedly: bloom filter, hash table with fixed size values + ds's where access is local}
%\TODO{build a function as a service framework on this}
%\TODO{can this be used for other systems such as parallel processing
% based on actor models and message queues with efficient support for
%e.g. fan out}
% supporting read-only operations and even write operations with carefully
% created static buffers that can contain unsupported (those that allocate/deallocate)
% operations that the object has received after a call to initiate migration has
% already been made.
% big table can be implemented easily using Slope
% as opposed to the original methods where we only think about the movement of
% objects, here we think about movement of servers
