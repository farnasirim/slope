\chapter{Evaluation}
\label{chap:evaluation}

We go over a few use cases of Slope in real-world systems and present both
micro-benchmarks and full benchmarks for select applications. We also discuss
the metrics that the applications using Slope can measure to get a sense of
how much Slope is impacting their performance.

To measure times and calculate performance metrics globally in the cluster,
we synchronize the start time from one machine to all other machines in the
cluster, by estimating the round trip time between them, which we do by
calculating the median among multiple round trip time calculations. Round
trip times are approximately $5 {\mu}{s}$, a pessimistic upper bound for the
error in time synchronization.

To make sure our graphs are accurate, we run each configuration at least 5 times
and average out the results, and in some cases, drop the min and max values to
eliminate the outlying points. That means each point in each of our graphs is
the resulting value from the above calculation on multiple runs with the same
configuration. Even without eliminating the outlying points, the standard errors
of our measurements are negligible (i.e. multiple orders of magnitude smaller)
compared to the reported values, unless otherwise stated.

Looking at each sub-system in Slope, one can define certain metrics that
reflect if that sub-system is working efficiently. For example we measure
the elapsed time until the prefill operation completes. We also measure
other metrics which more directly impact application performance, such as
end to end migration delay or time during which the object is unusable at
either end.

\subsection{Migration friendliness of data structures}
Based on how the migration process and specifically how the prefill operation
works, objects which use their internal memory in a ``fixed'' manner, that is
without doing much memory allocation/deallocation, are very good candidates
for migration, since they can function seamlessly throughout the prefill phase,
by only inducing dirty page overhead.

Examples of these objects include bloom filters, where we have a fixed
array of bits the size of which always stays the same.
Similarly hash tables which use open addressing techniques such as cuckoo
hashing scheme for their collision resolution are also good candidates for the
same reason.
Apart from these objects which make the best-case scenario for Slope, we also
discuss more generic objects whose allocation/deallocation patterns are not
ideal.

\section{Case study: core metrics and STL objects}
\subsection{Migrating a vector with clean pages}
\label{sec:cleanvec}

\begin{figure}[tp]
    \begin{center}
        \input{bench-readonly.pgf}
    \end{center}
    \caption{Migration statistics of a clean vector}
    \label{fig:vectorreadonly}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \input{bench-readonly-hp.pgf}
    \end{center}
    \caption{Migration statistics of a clean vector (huge pages)}
    \label{fig:vectorreadonlyhp}
\end{figure}

In our simplest example, we create a \texttt{vector}, initialize it with the
pre-specified size and migrate it to the destination. Approximately $8$ lines
of code are required on each of the source and destination sides to reproduce
this operation, excluding lines that serve the purpose of gathering statistics.
\autoref{fig:vectorreadonly} depicts the results.

Naturally, the prefill phase takes up most of the transfer time, which grows
linearly by increasing the size of the object. The increasing gap between the
prefill duration and the end to end latency can be attributed to syscalls and
invocation of the signal handler, both of which increase linearly with the
number of pages that are transferred before or after the prefill phase. The
time it takes to turn over the ownership is not impacted by the
size of the object and oscillates between tens of microseconds and hundreds
of microseconds. This is expected as this step consists of a single RDMA SEND.

\autoref{fig:vectorreadonly} shows that we are using about a hundredth of our 100 Gbps
network bandwidth and that the prefill phase is highly CPU bound. To use the
network bandwidth more effectively, we can increase the granularity of our
memory allocation unit by using huge pages. \autoref{fig:vectorreadonlyhp} shows
how using huge pages allows us to use around a fifth of our network bandwidth.
Furthermore, a 500-fold decrease in the number of allocation units decreases
the overal CPU cycles we spend for per-page operations in the memory allocator
and the control and data planes.

\subsection{Migrating a vector while dirtying all of its pages}
\begin{figure}[tp]
    \begin{center}
        \input{bench-writeall.pgf}
    \end{center}
    \caption{Migration statistics of a vector with all pages dirty}
    \label{fig:vectorwriteall}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \input{bench-writeall-hp.pgf}
    \end{center}
    \caption{Migration statistics of a vector with all pages dirty (huge pages)}
    \label{fig:vectorwriteallhp}
\end{figure}

In this micro-benchmark, we create a vector, dirty all of its pages after the
prefill phase has finished, and then finalize the transfer. Compared to the
clean scenario in \autoref{sec:cleanvec}, we need to spend extra time to
retransfer the dirty pages. As we would expect, the time it takes to turn over
the object ownership remains unchanged, however based on the usage of the
application, there will be a period during which the object is read-only on the
sender's side and the writes will be delayed on the receiver's side. We
explore the implications of this in \autoref{sec:evalmigfriendly} and
\autoref{sec:evalgenericobj}.

Although the same set of pages are sent/received during the prefill phase and
transferring of the dirty pages, the former takes considerably longer. This
happens because we do not need to pin the object memory to physical memory all
over again on each side.

Similar to the case with a read-only vector, we repeat the experiment with huge
pages. \autoref{fig:vectorwriteallhp} shows the result. The result resembles
those in \autoref{sec:cleanvec}, except here the elapsed time excluding the
prefill duration and the dirty page transfer duration is still significant. Most
of this time is contributed by the source machine having to execute the signal
handler while looping over the pages and dirtying them, after the
prefill phase and before the final transfer phase.

\section{Case study: Bloom filter}
\label{sec:evalmigfriendly}

size: 800000000, 383 pages, 256 of which dirty
latency compared to readonly: 1.4 end to end, latency not changed compared
to the writeall case.

200000 4KB pages, 60\% dirty

\begin{figure}[tp]
    \begin{center}
        \input{bench-bloomfilter.pgf}
    \end{center}
    \caption{Migration timeline of a Bloom filter}
    \label{fig:bloomfilter}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \input{bench-bloomfilter-hp.pgf}
    \end{center}
    \caption{Migration timeline of a Bloom filter (huge pages)}
    \label{fig:bloomfilterhp}
\end{figure}

\section{Case study: generic objects}
\label{sec:evalgenericobj}

\begin{figure}[tp]
    \begin{center}
        \input{bench-map.pgf}
    \end{center}
    \caption{Migration timeline of a map}
    \label{fig:map}
\end{figure}

\begin{figure}[tp]
    \begin{center}
        \input{bench-map-hp.pgf}
    \end{center}
    \caption{Migration timeline of a map (huge pages)}
    \label{fig:maphp}
\end{figure}


c++ map

23/24 dirty

5400/11900 4k



% \TODO{this model encourages fixed objects, where you don't need to allocate repeatedly: bloom filter, hash table with fixed size values + ds's where access is local}
%\TODO{build a function as a service framework on this}
%\TODO{can this be used for other systems such as parallel processing
% based on actor models and message queues with efficient support for
%e.g. fan out}
% supporting read-only operations and even write operations with carefully
% created static buffers that can contain unsupported (those that allocate/deallocate)
% operations that the object has received after a call to initiate migration has
% already been made.
% big table can be implemented easily using Slope
% as opposed to the original methods where we only think about the movement of
% objects, here we think about movement of servers
