\chapter{Migratable Applications}
\label{chap:migratableapps}

In this chapter we introduce migration-friendly applications and pinpoint
their properties. Later we conclude that
we must somehow step around serialization/deserialization of these objects for
performance and programmability reasons, and ``migration'' becomes
synonymous with ``sending without serialization/deserialization''.
We also discuss how the
application code interacts with these objects before or after the migration.
Takeaways from this chapter will be the design goals and the functional
requirements of a migration framework.
%
%speculate api
%
%possible usecases in distributed applications
%
%design goals and important metrics
%
%where do we need to be careful

% say these are the things we need to do, lay out some requirements,
% balance tradeoffs (e.g. access at source vs how quickly do the mig,
% track allocs

\section{Migration-friendly applications}
\label{sec:themigfriendlyapps}
In object-oriented partitionable applications, objects define the
computations and their presence on a machine dictates where the
computations will run. In a distributed hash table, a hash table partition
is such an object. We can distribute the objects in the cluster to balance
the loads that they impose on the machines.

\autoref{fig:designgoalspluggable} shows a high-level sketch of a
migration-friendly application which consists of symmetric instances
running on the machines in a cluster. The application can internally
divide its units of work into partitions that can be operated or
managed independently.
This partitioning scheme could be natural in the data, like
partitioning user data based on the user id, or it could be driven
by need for load balancing, such as randomly sharding the data
in a transaction processing system.


\begin{figure}[t]
\centering

\ensurepdffromsvg{design-goals-pluggable.drawio}
\includegraphics[width=0.5\textwidth]{design-goals-pluggable.drawio}
\caption{
    Migration-friendliness of an application with partitionable workload
}
\label{fig:designgoalspluggable}
\end{figure}

We call these ``listen and serve'' applications. They consist of multiple
shards, each of which can function on their own. After each shard is
initialized, it will indefinitely execute its ``listen and serve'' routine,
exposing an API through which the other sub-systems of the application
will communicate with that particular shard.

An immediate gain of such an application from the migration platform is the ability
to load balance. Given that the application can decompose its data and processing into
partitions that are small enough, it can use the migration platform to
spread out its load evenly across the cluster. However the migration platform must be
able to accomplish this quickly. For example if the workload of the application
is dynamic, meaning the need to rebalance the partitions arises frequently,
slow migration times will harm the overall performance of the system.

Such a platform also reduces the task of designing the \emph{whole system} to designing an
    \emph{application partition}
    .  Furthermore if we can do the migration completely
    seamlessly, we free the system designer from the burden of
    writing code for serializing and deserializing complicated objects.

A randomly sharded distributed key-value store is
an example of listen and serve applications, where the
partitions of the key-value store naturally map to the partitions in this
model. In the simplest case, each partition listens to serve the incoming
get and set requests. Each partition works independently and would continue to
work if we somehow moved it to a different machine.


\subsection{Core properties of a migration framework}
\label{subsec:coreprops}
Based on the observations in \autoref{sec:themigfriendlyapps}, we define
migration to stand for ``transferring an object to a
different machine without serialization/deserialization overheads, through an API that does not
propagate low level networking details to the core application logic''.
A
migration framework is the environment that makes this possible.

\paragraph{Real-time performance:}
We define \emph{migration} delay to be the elapsed time since the start of the
migration of an object until it is successfully migrated to its
destination. We need to keep the migration delay small since such a delay
is likely to cause a performance penalty for the application.

On the other hand, Minimizing end to end latency may not be the main priority of all applications.
For example in the case of virtual machine migration, Liu \textit{et al}$.$ \cite{vmliu}
propose a pre-copy approach resulting in a longer migration time than a stop
and go strategy, but their approach keeps the VM responsive during the transfer.
In Slope, our goal is to minimize the amount of time during which the
transferred object is inaccessible on both machines.

\paragraph{Pluggability and Programmability:}
A fast migration mechanism comes at a cost. Each application
can theoretically hand craft and micro-optimize its networking protocol,
but this will complicate the design of the application by propagating
the complexities of the low level protocol into its core.
This will make it difficult to maintain and improve the
application since a small change in how the application does one thing
may break the preconditions of the custom networking protocol.

We therefore aim for zero modifications to the
internals of the applications and minimal additions to the API of their
units of computation. That means if the system designer has already come up
with a way to partition data and computation into units that can run
independently on possibly different machines, it should be easy to
integrate the migration platform into the system to make those units
migratable and balance the application load. Furthermore Similarly the
application programmer should not need to worry about breaking the communication
protocol when she makes changes to the internal algorithms in the system.

\section{Computation model and API}
\label{sec:api}

\begin{figure}[t]
\begin{lstlisting}
template <typename T>
class ControlPlane {
 private:
  std::function<void(mig_ptr<T>)> run_;
 public:
  ControlPlane(std::function<void(mig_ptr<T>)> run): run_(run) { }
  MigrationOperation migrate(mig_ptr<T> p, Node destination);
  void accept(mig_ptr<T> p);
};
\end{lstlisting}
\caption{
    Slope's control plane
}
\label{fig:controlplane}
\end{figure}

All applications interface to Slope through Slope's \emph{control plane}.
\autoref{fig:controlplane} presents a simplified view of the API of the
control plane. When created, a \texttt{ControlPlane<T>} object can be
made aware of objects of type \texttt{T} through the use of the \texttt{accept}
method. In the distributed hash table case \texttt{T} could be a hash table
partition. To migrate an object, the application will call the \texttt{migrate}
method. The caller will then carry out the migration operation in multiple
steps using the returned \texttt{MigrationOperation} object.

Certain metadata of the objects are required during the migration
operation. To keep track of these, we wrap objects in
migratable pointers (\texttt{mig\_ptr<T>}) before passing them to the control
plane. migratable pointers are discussed in detail in \TODO{reference}.

In addition to sending objects to other machines, the control plane is also
responsible for receiving objects. Since objects can be migrated at any point
in time, we must be able to handle incoming objects asynchronously. Upon
creation, the control plane creates a thread to process incoming migrations.

To give the applications control over what happens to an object after it is
received, slope will call the function object \texttt{run\_}, passing in the
received object. The application programmer passes this function to Slope upon
constructing the control plane. In addition to being called when an object is
received, the \texttt{run\_} function is also called when we introduce an object
into the control plane by calling \texttt{accept}. The reason for this behavior
is that \texttt{run\_} is responsible for initializing (``running'') a partition.
This allows the partitions to be seamlessly initialized on the machine that they
are created.


In the hash table scenario, a typical
implementation of the \texttt{run\_} function would register the newly received
partition in the routing system, such that it can answer the incoming queries.
As a result each application instance can receive hash table partitions from
other instances and will correctly add them to the circuit to listen for queries.


\begin{figure}[tp]
\begin{lstlisting}
int main() {
    mig_ptr<HashTablePartition> partition;
    partition->put("id", get_current_machine_id());

    HashTableControlPlane<HashTablePartition> control_plane(
        [](mig_ptr<HashTablePartition> partition) {
            std::cout << partition->get("id") << std::endl;
        }
    );
    HashTableControlPlane.accept(partition);

    auto peer_id = get_current_machine_id() == "1" ? "0" : "1";
    auto mig_op = HashTableControlPlane.migrate(partition, peer_id);

    // carry out the migration using mig_op ...

    while (true) { /* wait */ }
}
\end{lstlisting}
\caption{
    Simple usage of Slope's control plane
}
\label{fig:sketchmain}
\end{figure}

To show how the API of the control plane is used, we sketch a simple application
which makes use of Slope's control plane.
\autoref{fig:sketchmain} shows the code for this application. Two machines
named ``1'' and ``2'' run the same code. On line 2, we create a hash table
partition, wrapped in a migratable pointer and add the key ``id'' to it, with its
value set to the machine id. Therefore after executing this line
\texttt{partition} will contain $\{"id": "1"\}$
on machine ``1'' and $\{"id": "2"\}$ on machine ``2''.

On line 5, the machines create their
control planes. For demonstration, we pass in a lambda as the \texttt{run\_}
function to print the hash table value for key ``id'' when a hash table
partition is received. After creating the control plane, the application can
asynchronously receive hash table partitions or use the control plane to
send object to other machines.

On line 10, each machine introduces its \texttt{partition} object to the
migration platform by calling the \texttt{accept} function which in turn calls
the \texttt{run\_} which we had previously passed to the control plane. Therefore
after calling accept, each machine can see its own ID written to stdout.

On line 12 each machine calculates the ID of its peer and on line 13, creates
a migration object for sending
its own \texttt{partition} object to the peer. After carrying out the migration
operation, each object will eventually be
received by the receiver thread of the control plane on the opposite machine.
Each object will be passed to \texttt{run\_} after it reaches the destination,
meaning that each of the machines will now see the ID of their peer written
to their stdout.
