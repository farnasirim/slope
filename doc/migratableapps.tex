\chapter{Migratable Applications}
\label{chap:migratableapps}

In this chapter we introduce migration-friendly applications and pinpoint
common properties among them. These speculations almost inevitably lead
us towards coming up with a certain computation model for these
applications.
The functional and performance requirements of this computation model will
ask for certain features from a hypothetical migration framework. As the
takeaway from this chapter we will infer the goals that we must pursue
in designing Slope to address those requirements.

speculate api

possible usecases in distributed applications

design goals and important metrics

where do we need to be careful

say these are the things we need to do, lay out some requirements,
balance tradeoffs (e.g. access at source vs how quickly do the mig,
track allocs

\section{Migration-friendly applications}
Distributed applications refer to a large family of applications with
different functionalities and requirements. From distributed transactions
engines and RPC frameworks to data pipelines and graph processing systems,
we must identify the
applications which can naturally play along with the notion of migrating
slices of their work to their peers to achieve better load balancing.

However instead of taking a vertical slice of applications from this range
to target one of these domains, a horizontal slice possibly spanning
multiple of the domains can help us better identify the core
properties of migratable applications.

\subsection{Migratable partitions}

\autoref{fig:designgoalspluggable} shows our first sketch of a
migration-friendly application which consists of symmetric instances
running on the machines in a cluster. The application can internally
partition its units of work into partitions that can be operated or
managed either independently, or less strictly,
with well defined and infrequent cross-communication through APIs.
This partitioning scheme could be natural, like
partitioning user data data based on the user id, or it could be driven
by a pure need for load balancing, such as randomly sharding the data
in a transaction processing system.


\begin{figure}[t]
\centering

\ensurepdffromsvg{design-goals-pluggable.drawio}
\includegraphics[width=0.5\textwidth]{design-goals-pluggable.drawio}
\caption{
    The migration platform allows the already partitioned units of
    data/computation to be migrated across machines. Application partitions
    are the self-sufficient, independent units of work and therefore the
    migratable objects in this system.
}
\label{fig:designgoalspluggable}
\end{figure}

An immediate gain of the application from this model is load balancing,
while retaining locality of access and in case of repeated requests to
the applications, cache locality.
Given that the application can decompose its data and processing into
partitions that are small enough, it can use the migration platform to
spread
out its load evenly across the cluster. In time-sensitive applications
if the load is dynamic,
meaning the need to rebalance the workload arises frequently,
then the migration platform must also work in real-time.

In these applications each shard or partition will likely provide the
full API of the whole system. In \autoref{fig:designgoalspluggable},
if the workload did not contain partitions 2 and 3, then the only
    difference in the transition from top to bottom would have been the
    thin API wrapper around its partition which is responsible for tasks
    such as routing the requests to the correct machine. Otherwise
    Partition 1 would look like the whole application from the outside.

Therefore apart from the run-time load balancing benefit, the
    migration platform also simplifies a part of the development of the
    application: the system designer needs to program to an
    \emph{application partition}, as opposed to programming to the
    \emph{whole system}. This separation of concern,
    by definition breaks the dependency between how
    any shard handles a request to its API and what the rest of the
    platform does, including how request routing or replication is done
    in the application. Note that the need for request routing or
    replication are not implications of the partitioned computation
    model: we had to go distributed already to meet the requirements of
    the big workloads, but in a partitioned model we can think about those
    in isolation without propagating their complications to the
    core of the application.


\subsection{Migratability as a transport}
With certain performance qualifications, a migration framework can act as
an intra-cluster transport. Example use cases for such a transport arise
in application level (or layer 7 in OSI terms) routers in heterogeneous
computation environments. \autoref{fig:migrationtransport} depicts
a simple example of this use case. 

\begin{figure}[t]
\centering

\ensurepdffromsvg{migration-transport.drawio}
\includegraphics[width=0.8\textwidth]{migration-transport.drawio}
\caption{
    Using a migration platform as the transport in a system with
    application level routing/load balancing. The system needs to execute
    the data parallel computation portion of each task on the machine
    which has a GPU. A request/job is the
    migratable element in this scenario. Each request is independent
    from other requests. Each request is also self sufficient as its
    state identifies what action and by which machine type must be
    executed on the object. Each node executes its share of work
    on the request object based on its current state, updates the state by
    writing to the object memory, and migrates it to the next hop based
    on the state.
}
\label{fig:migrationtransport}
\end{figure}

From a performance point of view, this makes little difference to the
application if there is serialization involved. In fact we could have
even framed
serialization and deserialization and RPC as a migration framework. If we
could do the migration in a ``pure'' way, meaning we could ideally
eliminate the serialization and deserialization, limiting what is sent
over the wire between each pair of servers to a minimum, the applications
performance has a chance of improvement. If we were able to hand off
objects between servers in a way that is not different from calling a
function on the same server, the two servers will in essence look
``closer'' to each other both from a performance and a programmability
point of view. The former will result in improvements in run-time and
the latter will empower system designers.

The difference between this computation model and a general
request-response or RPC model is that the unit of work is a
well defined object and each step in the computation can be
efficiently modeled as doing certain computation work and then taking
the object to the next state. This will rarely resemble the computation
model of an application that is modeled as a number of completely separate
services, or how a mobile client would communicate with the application
server.

From a different angle, this model allows us to choose a tightly
coupled set of performance critical tasks in an application with
heterogeneous data or computation requirements, and model their
communication using a framework which hides low level networking details
and takes away the need to serialize or deserialize payloads.
This is an improvement in programmability. With little development
effort and no complications in the design of the application,
we can build, maintain and
extend sub-systems which can carry out tasks across multiple machines.

\subsection{Core properties of a migration framework}

The function of a migration platform is to provide a mechanism to allow
applications to seamlessly move their data structures and their
corresponding operations across their instances.

\paragraph{Real-time performance:}
We define migration delay to be the elapsed time since the start of the
migration of an object until it is successfully migrated to its
destination. Based on the two application types that were discussed,
the most
important feature of a migration framework is its ability to keep the
migration delay small to allow migrations to happen in real-time.
However this is not particularly insightful in designing such a system.
All applications will prefer a smaller end-to-end migration delay
if all of their other metrics are left unchanged. Similarly exceedingly
    large migration delays will interfere with the performance of
    virtually all applications. Furthermore, directly measuring this
    duration and pursuing its optimization as the main performance goal
    may fail to reflect how the migration process impacts the application.
    For example some applications may prioritize responsiveness and
    would trade off
    some of their throughput during the migration process to keep their
    tail latencies low.  Therefore it makes more sense to define migration
    performance from the view point of the application based on how
    \emph{its} performance metrics degrade as a result of the migration.
    While there is value in keeping the migration delay small, we must
    prioritize maximizing the \emph{usefulness} of the application in
    the distributed setting during the migration.

\paragraph{Pluggability and Programmability:}
A fast migration mechanism comes at a cost. After all, each application
could theoretically hand craft and micro-optimize its networking protocol,
but this will complicate the design of the application by propagating
the complexities of the low level protocol into the core of the
application. This will make it difficult to maintain and improve the
application since a small change in how the application carries out its
task may break the preconditions of the custom protocol. Therefore as the
second core requirement of migration platforms, 
we will aim for zero modification to the
internals of the application and minimal addition to the API of the
units of computation. That means if
the system designer has already come up with a way to partition
data and computation into units that can be run independently on
possibly different machines, the task of integrating the migration
platform into the system to make those units migratable should be trivial.



\section{Computation model and API}
\label{sec:api}
It is not yet clear how a migration platform will be used by the
applications. How does the source machine deals with the fact that it
no longer possesses an object? How will the destination receive an object?
How would the destination application know what to do with an object
that it has just received?

In a simple artificial case where we are migrating a singleton across
the nodes in the cluster, handing its ownership to the cluster nodes one
at a time, these issues might sound trivial. However in more complicated
cases where there can be multiple types of migratable objects in the
system with varying quantities, we need to build a systematic model that
explains what we can do with migratable objects and how the participating
machines can do it.

In this section we will revisit the two types of applications that
we previously introduced and discuss how they can interface with a
hypothetical migration platform.

\subsection{Listen and serve model}
the first type of application that we discussed can be thought of as a
part of more general family of distributed applications that we call
listen and serve applications.
Such systems consist of multiple partitions or shards, each
of which can function on their own. After each shard is initialized, it
will indefinitely execute the "listen and serve" routine,
exposing an API through which the other sub-systems of the application
will communicate with that particular shard. Since each of these shards
operates independently from the other ones and is only
responsible for a certain part of the workload, the application
must also provide a routing mechanism to
deliver requests to the correct shard. A randomly sharded distributed
key-value store is an example of this type of applications, where the
partitions of the key-value store naturally map to the partitions in this
model. In the simplest case, each of the partition waits for the incoming
get and set requests, which corresponds to listen and serve in this model.
The application routes requests by keeping track of which machine
is responsible for which set of keys (e.g. through a consistent hashing
scheme).

\subsection{State machine model}
Generalizing the second type of application will lead us to modeling
tasks as state machines. In this model each task consists of an
independent object in conjunction with the set of operations that must be
executed on the object, which result in carrying on the task. Upon
receiving of the object, the machine will run the routine that the current
state of the object dictates. At the end of the routine based on the, new
state of the object, it might be migrated to another machine. In
\ref{fig:migration/transport} the routine in the ``gpu'' state could be:
``If on a non-GPU machine, migrate to a GPU machine with the same state,
otherwise perform the computation and move to the ready state''. This
model can be useful for handling tasks which depend on heterogeneous
hardware for acceleration. Using this model, any machine-local
resource in the cluster can be made available to the whole cluster over
a migration-based transport.

describe the \texttt{run()} function and describe the ``thread of execution''.
\TODO{...}

describe threading model

\TODO{figure for per core and how threads interact with them}

\TODO{fill}
Fundamentally every server has to be able to host an incoming object.
Since everything is async, this means that the object must know how to
"register" itself upon arrival in the remote node. Be it exposing a particular
api on a unix domain socket, or adding itself as an observer or worker to a blah.

each control plane has a fixed type

applications can intelligently use the time until ownership is transferred by
supporting read-only operations and even write operations with carefully
created static buffers that can contain unsupported (those that allocate/deallocate)
operations that the object has received after a call to initiate migration has
already been made.

